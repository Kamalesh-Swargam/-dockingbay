{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kamalesh-Swargam/-dockingbay/blob/main/comfyui_colab_Flux.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the [ComfyUI](https://github.com/comfyanonymous/ComfyUI) repo and install the requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bbbbbbbbbb"
      },
      "outputs": [],
      "source": [
        "!git clone -q https://github.com/comfyanonymous/ComfyUI\n",
        "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu130\n",
        "%pip install -q -r ./ComfyUI/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some Helpful Custom Nodes\n",
        "!git clone -q https://github.com/ltdrdata/ComfyUI-Manager /content/ComfyUI/custom_nodes/comfyui-manager\n",
        "\n",
        "!git clone -q https://github.com/ClownsharkBatwing/RES4LYF.git /content/ComfyUI/custom_nodes/RES4LYF\n",
        "!git clone -q https://github.com/rgthree/rgthree-comfy.git /content/ComfyUI/custom_nodes/rgthree-comfy\n",
        "!git clone -q https://github.com/yolain/ComfyUI-Easy-Use.git /content/ComfyUI/custom_nodes/ComfyUI-Easy-Use"
      ],
      "metadata": {
        "id": "0a9tsc6ym4F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download the vae and dual text encoder needed for FLUX - (Compulsory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb9xz1ViENoO"
      },
      "outputs": [],
      "source": [
        "!wget -q --show-progress https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors -P /content/ComfyUI/models/text_encoders/\n",
        "!wget -q --show-progress https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors -P /content/ComfyUI/models/text_encoders/\n",
        "!wget -q --show-progress https://huggingface.co/cocktailpeanut/xulf-dev/resolve/main/ae.safetensors -P /content/ComfyUI/models/vae/\n",
        "\n",
        "# 8-step distilled lora for fast inference on dev models\n",
        "# !wget -q --show-progress https://huggingface.co/alimama-creative/FLUX.1-Turbo-Alpha/resolve/main/diffusion_pytorch_model.safetensors -P /content/ComfyUI/models/loras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWglWy21uVpP"
      },
      "source": [
        "\n",
        "The **FP8 version** runs smoothly on the free tier (without LoRA), but if you want to use it with a LoRA, it requires more resources than the free Colab tier provides.  \n",
        "In that case, you can opt for the [gguf](https://huggingface.co/collections/QuantStack/flux-ggufs-68264cfc663d50c418940b30) quantized versions or the [Nunchaku](https://huggingface.co/nunchaku-tech/models) versions instead.\n",
        "\n",
        ">**Note** : You need only one , either a gguf quantized model (under 10GB) or the Nunchanku model\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dvla7XiuVpN"
      },
      "outputs": [],
      "source": [
        "# %pip install --upgrade gguf\n",
        "# !rm -rf ./custom_nodes/ComfyUI-GGUF\n",
        "# !git clone https://github.com/city96/ComfyUI-GGUF /content/ComfyUI/custom_nodes/ComfyUI-GGUF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Force install the GGUF Custom Node (Bypassing Manager)\n",
        "!git clone https://github.com/city96/ComfyUI-GGUF /content/ComfyUI/custom_nodes/ComfyUI-GGUF\n",
        "\n",
        "# 2. Move your model to the correct \"Unet\" folder (Where GGUF nodes look)\n",
        "# (Your screenshot showed it was in diffusion_models, we move it to be safe)\n",
        "!mv /content/ComfyUI/models/diffusion_models/flux1-dev-Q4_0.gguf /content/ComfyUI/models/unet/flux1-dev-Q4_0.gguf"
      ],
      "metadata": {
        "id": "OAix5H75DB1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run ComfyUI with ngrok"
      ],
      "metadata": {
        "id": "8XPOPExF_tly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import subprocess, socket, time\n",
        "from google.colab import userdata\n",
        "\n",
        "NGROK_TOKEN = userdata.get(\"NGROK_TOKEN\")\n",
        "\n",
        "# Set ngrok token\n",
        "!ngrok config add-authtoken $NGROK_TOKEN\n",
        "\n",
        "# Start ComfyUI in background\n",
        "subprocess.Popen([\"python\", \"/content/ComfyUI/main.py\", \"--dont-print-server\"])\n",
        "\n",
        "# Wait until port is open\n",
        "port = 8188\n",
        "while True:\n",
        "    try:\n",
        "        sock = socket.create_connection((\"127.0.0.1\", port), timeout=2)\n",
        "        sock.close()\n",
        "        print(\"‚úÖ ComfyUI server is running on port\", port)\n",
        "        break\n",
        "    except OSError:\n",
        "        print(\"‚è≥ Waiting for ComfyUI to start...\")\n",
        "        time.sleep(2)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8188, bind_tls=True)\n",
        "print(\"üåê Public URL:\", public_url)\n"
      ],
      "metadata": {
        "id": "MTI7AYf1_ExB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gggggggggg"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}